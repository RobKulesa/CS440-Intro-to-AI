%! Author = Robert Kulesa, Daniel Liu, Michael Li
%! Date = 10/5/2021

% Preamble
\documentclass[11pt]{article}

% Packages
\usepackage{amsmath}

% Document
\begin{document}
    \begin{titlepage}
        \begin{center}
            \vspace{1cm}

            \Huge
            \textbf{Face and Digit Classification}

            \vspace{0.5cm}
            \LARGE
            Final Project

            \vspace{1cm}

            \textbf{Michael Li - 192008938}

            \textbf{Daniel Liu - 184007283}

            \textbf{Robert Kulesa - 185009892}


            \vfill


            \vspace{0.8cm}

            \Large
            CS440 Fall 2021\\
            Professor Boularias\\
            Rutgers University - New Brunswick\\
            December 14, 2021

        \end{center}
    \end{titlepage}

    \begin{center}
        \Large
        \textbf{Classifier 1 - Perceptron}
    \end{center}
    \normalsize

    \begin{center}
        \Large
        \textbf{Classifier 2 - Naive Bayes}
    \end{center}
    \normalsize
    The Naive Bayes classifier is based off of the Bayes Theoreum, which is defined as:\\
    
    \[P(y|X) = \frac{P(X|y)*P(y)}{P(X)}\]
    
    It functions by calculating the posterior probability $P(y|X)$ of each possible class based off of frequency distributions observed in the training set and choosing the class with the highest posterior probability as its prediction. This specific implementation of Naive Bayes used each pixel in the training images as an input, and calculated the likelihood of a each pixel being "on" or "off" given a class label (i.e. face, not face). Typically the posterior probability is calculated by multiplying the prior probability of a class  with the likelihood of each feature, but since the number of features was so high the log-sum of likelihoods and prior probabilities was used instead to avoid arithmetic underflow when calculating posterior probabilities. Additionally, calculating the denominator $P(X)$ is a redundant calculation since it does not change the relative difference between likelihood scores. As a result, the posterior probability equation used by this implementation of Naive Bayes is the following:\\ 
    \[P(y|X) = log(P(y)) + log(P(X|y))\]
    
    A smoothing factor of +1 was also applied to each likelihood calculation to avoid a zero probability prediction if a given feature distribution is not represented well in the training data.
    
    \begin{center}
        \Large
        \textbf{Classifier 3 - K Nearest Neighbor}
    \end{center}
    \normalsize
    The K-Nearest Neighbor (KNN) Classifier works by taking a test sample, calculating its
    distance to all training samples, and classifying the test sample as whichever label
    the majority of the $K$ closest samples hold.\\\\
    \{include section on feature extraction if we all do it differently\}\\\\
    Many distance functions can be used, and each has their own advantages and disadvantages for
    different datasets, such as speed and memory requirements.\\\\
    For the face dataset, this implementation of KNN uses the cosine distance as the distance function.
    Mathematically, the cosine distance is the cosine of the angle between two vectors in n-dimensional space.
    The cosine distance of vectors $q$ and $p$ can be calculated as:
    \[dist\left(q, p\right) = 1-\cos(\theta) = 1-\frac{q \cdot p}{\|q\|\|p\|}\]
    For the digits dataset, this implementation of KNN uses the euclidean distance as the distance function.
    Mathematically, the euclidean distance is the length of the line segment that connects two points in n-dimensional space.
    The euclidean distance of vectors $q$ and $p$ can be calculated as:
    \[dist\left(q, p\right) = \sqrt{\sum_{i=1}^{n}\left(q_i-p_i\right)^2}\]

    \begin{center}
        \Large
        \textbf{Classification Model Comparison}
    \end{center}
    \normalsize
    
    The Naive Bayes classifier was able to achieve fast classification speeds by precalculating all possible likelihoods for each feature (pixel) in a given sample and class. Thus, when the predict() method is called, the model only needs to look up the likelihoods of each feature in a sample from a pregenerated table of likelihoods. The Naive Bayes model was able to classify the face test set in 0.576 seconds (150 samples) with an accuracy of 0.893 and the digit test set in 3.699 seconds (1000 samples) with an accuracy of 0.769.

    Testing accuracy of the classifier when trained on different percentages of training data was evaluated. The model randomly sampled a percentage of the training data 5 times at each stage and the means and standard deviations are displayed below. Testing accuracy increased with the amount of training data provided, with the greatest increase being observed when increasing from 10\% of the training data to 20\% of the training data.
    
    \begin{center}
    \begin{tabular}{||c c c||} 
     \hline
     \% Training Data & Mean Accuracy & Standard Deviation \\ [0.5ex] 
     \hline\hline
     10 & 0.487 & 0.001 \\ 
     \hline
     20 & 0.813 & 0.053 \\ 
     \hline
     30 & 0.8 & 0.067 \\ 
     \hline
     40 & 0.84 & 0.027 \\ 
     \hline
     50 & 0.863 & 0.01 \\ 
     \hline
     60 & 0.847 & 0.007 \\ 
     \hline
     70 & 0.88 & 0.007 \\ 
     \hline
     80 & 0.887 & 0.007 \\ 
     \hline
     90 & 0.88 & 0.007 \\ 
     \hline
     100 & 0.893 & 0.0 \\ 
     \hline
    \end{tabular}
    \end{center}

\end{document}
